

% ======================================================================
\unnsubsection{Code-Automation as a Standard in Modern Biosciences}
\
Beschreibe die Situation.
- Big Data in Biosciences
- what is big data, examples
- Define citable challenges:
- reproducibility crisis
- lack of tools

% - Semi big data
% - What is that: At the edge of managability
% - Tools to handle semi-big data are lacking:
% Either manual analysis or big data tools
% - Author defines semi-automation



In recent years, the biosciences have evolved dramatically, with a marked
increase in the volume and complexity of data generated
~\cite{yangScalabilityValidationBig2017,ekmekciIntroductionProgrammingBioscientists2016}.
This transformation necessitates robust software tools, many of which require
coding skills to use effectively. Here we summarize standard tools used by
biosciences today and show their reliance on coding. The author argues that the
role of a modern independent researcher is now intertwined with coding skills
similar to a role of ``precision medicine
bioninformatician''~\cite{gomez-lopezPrecisionMedicineNeeds2019}.

Statistical analysis in biosciences has traditionally been reliant on
user-friendly tools like Excel and GraphPad Prism. While Excel by itself is
recognized as limited for complex data analysis
\cite{tanavaleeLimitationsUsingMicrosoft2016a, incertiYouStillUsing2019a},
GraphPad Prism offers more advanced statistical models .

However, increasingly demands more sophisticated
approaches as data sets grow in size and complexity.

R and Python scripts offer more efficient and
versatile solutions, enabling complex analyses with a few lines of code
\cite{rcoreteamLanguageEnvironmentStatistical2018,vallatPingouinStatisticsPython2018}.

Recognizing this trend, Microsoft has integrated a Python interpreter into Excel to
computations more accessible within a widely used platform \cite{AnnouncingPythonExcel2023}.

% A potential use-case for coding is statistical analysis. Most users rely on tools
% like Prism.
% However, Prism becomes impractical with large and complex datasets,
% as manual re-analysis is time-consuming. In contrast, scripts in R or Python
% allow for a single click to re-analyze the data. Recognizing this trend,
% Microsoft has recently implemented a Python interpreter into Excel, allowing
% execution of python code within cells. R, in particular, offers more advanced
% statistical models than Prism or Excel, which has led to R's widespread
% adoption.

Next-generation sequencing, such as bulk RNAseq, has become affordable, allowing
for larger sample sets during a single PhD project. This technology offers
advanced tools that are most efficiently used through scripting in R or Python.
In the absence of a dedicated statistician, researchers are compelled to learn
coding.

In gene ontology, tools such as Metascape facilitate the integration of vast
datasets and outputs multiple useful data visualizations. Metascape also
provides multiple excel sheets, containing all results, sometimes in a nested
format, which provides even further information that's adaptable for specific
hypotheses, but given the sheer amount of data, is impractical to analyze
manually.

since Metascape
returns large Excel sheets with complex nested information, a researcher without
coding skills requires manual work to adapt the results to specific research
hypotheses.

its true potential is unlocked only when researchers can
manipulate and analyze these data through scripting.

Modern gene ontology tools like Metascape offer powerful graphical user
interfaces. However, their effectiveness is only possible through standardizing
multiple large datasets.

The output from Metascape, large Excel sheets with
complex nested information, is more efficiently analyzed through scripting,
which is often necessary to adapt metascape results to specific research
hypotheses.



Image analysis is another area where coding skills are essential. ImageJ/FIJI, a
standard tool in the field, requires scripting for batch processing of multiple
images and automating multiple processing steps into a pipeline. While macros
can be recorded, understanding the underlying code is necessary for
troubleshooting and adapting the macro to new datasets.

In the field of protein structural biology, Pymol is a standard tool that also
has a Python command interface.

Similarly, artificial intelligence (AI), a
game-changer in biomedicine, primarily uses Python due to its extensive
libraries for machine learning and scientific computing. Python is also a
standard for integrative biomedicine simulations.

Finally, databases and repositories are essential for storing, retrieving, and
sharing data. Researchers need to understand common file formats to adhere to
standards that ensure re-usability and interoperability. Scripting helps
automate the process of formatting data for submission to these databases.

In conclusion, the integration of coding in bioscience research is not just a
trend but a necessity. As the field continues to evolve, the demarcation between
biologists and computational scientists blurs, underscoring the importance of
coding skills for the next generation of researchers. The ability to code is
fast becoming an indispensable asset, as integral to bioscience as traditional
laboratory skills.



%%%%%%%%%%%%%%%

% In the last decades, biosciences have made significant progress in generating
% vast amounts of data in shorter time spans
% ~\cite{yangScalabilityValidationBig2017}. Here, it is argued that research reliant
% on software more than ever. More so, the author of the thesis highly doubts that
% every modern researcher will be confronted with executing at least some lines of
% code during their career, or maybe even collect multiple commands in a
% self-written script.


% - The question is, can a modern PhD student fulfill their role as an independent
% researcher without coding skills? This is highly questionable, given that
% standard tools in the biosciences require a code interface:


% - Statistics? No. Requires Prism, but Prism becomes unpractical with large and
% complex datasets, since manual re-analysis has to be repeated manually, while
% scripts would allow for a single click to re-analyze the data. Hence, R or
% Python have become a standard, with R offering more advanced statistical models
% than Prism or Excel. Excel has recognized this development and
% introduced Python support

% - Next generation sequencing? Rnaseq has become cheap (bulk RNAseq), allowing for growing
% samplesets during one PhD project, while gaining in remarkable
% precision, even at single-cell level. It offers advanced tools, that are only
% used efficiently scripting in R or Python. If
% there's no dedicated statistician, researchers have no choice but to learn
% coding

% - Gene ontology: Modern tools like metascape offer a surprisingly powerful
% graphical user interface. Still this achievement is only possible through
% standardizing multiple large datasets. Also the output of metascape are large
% excel sheets with highly complex nested information. These that are unpractical
% to analyze manually by excel and are more efficiently analyzed through scripting
% to adapt to specific needs of research hypotheses.

% - Image analysis? ImageJ/FIJI is standard. However, there are bugs. Also,
% scripting is required for both batch processing of multiple images and also
% automating multiple processing steps into a pipeline. Of course, macros can be
% recorded, yet an understanding of the underlying code is required to
% troubleshoot and adapt the macro to new datasets.

% - Protein structurual biologists? Pymol is standard and also has a python
% command interface

% - Artificial intelligence (AI) has been a game changer in the field of
% biomedicine. The early development of AI itself was driven by radiology, where
% it was designed to detect pathologies in medical images. Today, Python is a
% standard because of its extensive libraries for machine learning and scientific
% computing.


% - Integrative biomedicine simulations? Python is a standard

% - In general, coding skills help improve a researchers general understanding of
% every software he uses. This is because the researcher can understand the
% underlying algorithms and assumptions of the software. Complex proprietary
% software to use state-of-the-art equipment (Zen, Imaris, FlowJo, etc.) are often
% black boxes. The researcher will be confronted with issues and has to make
% constant decisions if the error is on his side, or on the software side in order
% to think of feasable troubleshooting strategies. This is only possible if the
% researcher has a good understanding of the software. A basic understanding of
% code also opens the door to open-source software, allowing for state-of-the-art
% analysis techniques that are not available in proprietary software, but their
% installation and usage is often overestimated by researchers without coding
% skills, while coding skills allow for adaptations of this software to the
% specific needs of the researcher.

% - Furthermore, databases and repositories are essential for storing, retrieving,
% and sharing data. In order to adhere to standards that ensure re-usability and
% interoparability, researchers need to have an understanding of common file
% formats. Scripting also helps to automate the process of formatting data for
% submitting to these databases.

% Together, it remains highly questionable, if future scientists will be able to
% perform their role as independent researchers without minimum coding skills.
% In conclusion, the increasing role of software in biomedicine underscores the
% importance of computational skills for modern researchers. As the field
% continues to evolve, the ability to work with software will become even more
% critical.

%%%%%%%%%%%%%%%%



% Modern methods in molecular biology, biochemistry, and biomedicine, such as
% next-generation sequencing, mass spectrometry, and high-throughput screening,
% generate large volumes of data that require sophisticated software tools for
% analysis. For instance, bioinformatics software is essential for analyzing
% genomic and proteomic data, while image analysis software is routinely used in
% microscopy.


% Moreover, the rise of systems biology and integrative biomedicine, which aim to
% understand biological systems as a whole, has led to the development of complex
% computational models and simulation software. These tools are used to integrate
% and analyze diverse data types, from molecular to physiological data, and to
% predict the behavior of biological systems.


% In addition, software plays a crucial role in the management and sharing of
% biomedical data. Databases and data repositories are essential for storing,
% retrieving, and sharing data, while data standards and ontologies, which are
% often implemented as software libraries, are used to ensure that data is
% interoperable and reusable.


% Given this landscape, it is clear that researchers in the biosciences are
% confronted with complex software on a daily basis. Therefore, there is a growing
% need for researchers to acquire computational skills.

% Learning a programming
% language like Python can greatly benefit researchers by enabling them to
% automate tasks, analyze data more efficiently, and develop their own tools. This
% not only increases productivity but also fosters reproducibility and open
% science.


% In conclusion, the increasing role of software in biomedicine underscores the
% importance of computational skills for modern researchers. As the field
% continues to evolve, the ability to work with software will become even more
% critical.





% ======================================================================
\unnsubsection{How Code Quality Improves Scientific Reproducibility}
A main reason to write software is to define re-usable instructions for task
automation~\cite{narztReusabilityConceptProcess1998}. However, the complexity of
the code makes it prone to errors and can prevent usage by persons other than
the author himself. This is a problem for the general scientific community, as
the software is often the only way to reproduce the results of a
study~\cite{sandveTenSimpleRules2013}. Hence, modern journals aim to enforce
standards to software development, including software written and used by
biological researchers~\cite{smithJournalOpenSource2018}. Here, we provide a
brief overview of the standards utilized by \texttt{plotastic} that to ensure
its reliability and reproducibility by the scientific community~\cite{pengReproducibleResearchComputational2011}.

Modern software development is a long-term commitment of maintaining and
improving code after initial release~\cite{boswellArtReadableCode2011}. Hence,
it is good practice to write the software such that it is scalable, maintainable
and usable. Scalability or, to be precise, structural scalability means that the
software can easily be expanded with new features without major modifications to
its architecture \cite{bondiCharacteristicsScalabilityTheir2000}. This is
achieved by writing the software in a modular fashion, where each module is
responsible for a single function. Maintainability means that the software can
easily be fixed from bugs and adapted to new requirements
\cite{kazmanMaintainability2020}. This is achieved by writing the code in a
clear and readable manner, and by writing tests that ensure that the code works
as expected~\cite{boswellArtReadableCode2011}. Usability is hard to
define~\cite{brookeSUSQuickDirty1996}, yet one can consider a software as usable
if the commands have intuitive names and if the software's manual, termed
``documentation'', is up-to-date and easy to understand for new users with
minimal coding experience. A software package that has not received an update
for a long time (approx. one year) could be considered abandoned. Abandoned
software is unlikely to be fully functional, since it relies on other software
(dependencies) that has changed in functionality or introduce bugs that were not
expected by the developers of all dependencies. Together, software that's
scalable, maintainable and usable requires continuous changes to its codebase.
There are best practices that standardize the continuous change of the codebase,
including version control, continuous integration (often referred to as CI), and
software testing.

Version control is a system that records changes to the codebase line by line,
allowing the documentation of the history of the codebase, including who made
which changes and when. This is required to isolate new and experimental
features into newer versions and away from the stable version that's known to
work. The most popular version control system is Git, which is considered the
industry standard for software development~\cite{chaconGitBook2024}. Git can use
GitHub.com as a platform to store and host codebases in the form of software
repositories. GitHub's most famous feature is called ``pull request''. A pull
request is a request from anyone registered on GitHub to include their changes
to the codebase (as in ``please pull this into your main code''). One could see
pull requests as the identifying feature of the open source community, since it
exposes the codebase to potentially thousands of independent developers,
reaching a workforce that is impossible to achieve with closed source models
used by paid software companies.

Continuous integration (CI) is a software development practice in which
developers integrate code changes into a shared repository several times a
day~\cite{duvall2007continuous}. Each integration triggers the test suite,
aiming to detect errors as soon as possible. The test suite includes building
the software, setting up an environment for the software to run and then
executing the programmed tests, ensuring that the software runs as a whole.
Continuous integration is often used together with software branches. Branches
are independent copies of the codebase that are meant to be merged back into the
original code once the changes are finished. Since branches accumulate multiple
changes over time, this can lead to minor incompatibilities between the branches
of all developers (integration conflicts), which is something that CI helps to
prevent.

Continuous integration especially relies on a thorough software testing suite.
Software testing is the practice of writing code that checks if the codebase
works as expected~\cite{10.5555/2161638}. The main type of software testing is
unit testing, which tests the smallest units of the codebase (functions and
classes) in isolation (\autoref{lst:unit_test}).

\def\mycaption{ Example of an arbitrary python function and its respective unit
    test function. The first function simply returns the number 5. The second
    function tests if the first function indeed returns the number 5. The test
    function is named with the prefix ``\texttt{test\_}'' and is placed in a
    file that ends with the suffix ``\texttt{\_test.py}''. The test function is
    executed by the testing framework \texttt{pytest}. Note that code after
    ``\texttt{\#}'' is considered a comment and won't be executed.}
\begin{lstlisting}[
    language=Python, 
    style=pythonstyle,
    label=lst:unit_test, 
    caption=\mycaption,
    ]
# Define a function called "give_me_five" that returns the number 5
def give_me_five():
    return 5
# Define a test function asserting that "give_me_five" returns 5
def test_give_me_five():
    assert give_me_five() == 5 
\end{lstlisting}

The quality of the software testing suite is measured by the code coverage, the
precision of the tests, and the number of test-cases that are checked. The code
coverage is the percentage of the codebase that is called by the testing
functions, which should be as close to 100\% as possible, although it does not
measure how well the code is tested. The precision of the test is not a
measurable quantity, but it represents if the tests truly checks if the code
works as expected. The number of test-cases is the number of different scenarios
that are checked by the testing functions, for example testing every possible
option or combinations of options for functions that have multiple options. The
most popular software testing framework for python is \texttt{pytest}, which is
utilized by \texttt{plotastic}~\cite{pytestx.y}.

Together, the standards of software development, including version control,
continuous integration, and software testing, ensure that the software is
scalable, maintainable, and usable. This is especially important for software
that is used by the scientific community, as it ensures that the software is
working as expected at defined versions years after publishing scientific
results.

% ======================================================================
% ======================================================================
\unnsubsection{Python as a Programming Language}
Here, we provide a general overview of the python programming language,
explaining terms like \textit{``type''}, \textit{``method''}, etc., in order to
prepare readers without prior programming experience for the following chapters.
We also describe the design principles of python to lay out the key concepts
that differentiate python compared to other programming languages. A more
detailed tutorial on python that's specialized for bioscientists is found
in~\citealt{ekmekciIntroductionProgrammingBioscientists2016}

\def\mycaption{ Example of
    readable python code. This one-line code returns the words (string)
    \texttt{'Hello, World!'} when executed. The command is straightforward and easy
    to understand.}
\begin{lstlisting}[
    language=Python, 
    style=pythonstyle,
    label=lst:readable,
    caption=\mycaption
    ]
print("Hello, World!")
# Output: Hello, World!
\end{lstlisting}

Languages such as python are considered \textit{``high-level''}, which means
that it is designed to be easy to read and write, but also independent of
hardware by hiding (\textit{``abstracting''}) underlying
details~\cite{PythonLanguageReference}. A key principle of python is the
emphasis on implementing a syntax that is concise and close to human language
(\autoref{lst:readable}, \autoref{lst:not_readable}).

\def\mycaption{ Example of less readable code written in the low-level
    programming language C. This code is doing exactly the same as the python
    code in \autoref{lst:readable}. The command is harder to understand because
    more steps are needed to access the same functionality, including the
    definition of a function}
\begin{lstlisting}[
    language=C, 
    style=defaultstyle,
    label=lst:not_readable, 
    caption=\mycaption
    ]
#include <stdio.h>
int main() {
    printf("Hello, World!");
    return 0;
}
// Output: Hello, World!
\end{lstlisting}

Furthermore, python is an \textit{interpreted} language, which means that the
code is executed line by line. This makes coding easier because the programmer
can see the results of the code immediately after writing it, and error messages
point to the exact line where the error occurred. This is in contrast to
\textit{compiled} languages, where the code has to be compiled into machine code
before it can be executed. The advantage of compiled languages is that the code
runs faster, because the machine code is optimized for the hardware.

Python automates tasks that would otherwise require an advanced understanding of
computer hardware, like the need for manual allocation of memory space. This is
achieved by using a technique called \textit{``garbage collection''}, which
automatically frees memory space that is no longer needed by the program. This
is a feature that is not present in low-level programming languages like C or
C++, that were designed to maximize control over hardware.

Another hallmark of python is its \textit{dynamic typing system}. In python the
type  is inferred automatically during code execution
(\autoref{lst:dynamic_typing}). This is in contrast to \textit{statically} typed
languages like C, where the type of a variable has to be declared explicitly and
cannot be changed during code execution
(\autoref{lst:static_typing})~\cite{PythonLanguageReference}.

\def\mycaption{ Example of dynamic typing in python. The variable ``\texttt{a}''
    is assigned the value 5, which is of type integer. The variable ``\texttt{a}''
    is then assigned the value ``\texttt{Hello, World!}'', which is of type string.
    Python allows dynamic re-assignment of variables with different types. Note that
    code after ``\texttt{\#}'' is considered a comment and won't be executed.}
\begin{lstlisting}[
    language=Python,
    style=pythonstyleNonbreaking,
    label=lst:dynamic_typing,
    caption=\mycaption,
    belowskip=-\vhalf % > Remove space because two listings are close
    ]
a = 5  # Type integer
a = 5.0  # Type float
a = 'Hello, World!'  # Type string
a = True  # Type boolean
a = False  # Type boolean
a = [1, 2, 3]  # Type list of integers
a = {'name': 'Regina'}  # Type dictionary
\end{lstlisting}

\def\mycaption{ Example of static typing in C. The variable ``\texttt{a}'' is
    declared as an integer (\texttt{int}), and can only store integers. The
    variable ``\texttt{a}'' is then assigned the value 5, which is an integer.
    The variable ``\texttt{a}'' is then assigned the value \texttt{'Hello,
        World!'}, which is a string. This results in a compilation error, because
    the variable ``\texttt{a}'' can only store integers. Note that code after
    ``\texttt{//}'' is considered a comment and won't be executed. }
\begin{lstlisting}[
    language=C,
    style=defaultstyleNonbreaking,
    label=lst:static_typing,
    caption=\mycaption,
    ]
int a;  // Declare type as integer
a = 5;
a = 'Hello, World!';  // Compilation error!
\end{lstlisting}

Dynamic typing makes python a very beginner-friendly language, since one does
not have to keep track of the type of each variable. However, this also makes
python a slower language, because the interpreter has to check the type of each
variable during code execution. Also, developing code with dynamic typing
systems is prone to introducing bugs (``type errors''), because it allows
unexperienced developers to convert variables from one type to another without
noticing, leading to unexpected behavior. Hence, larger python projects require
disciplined adherence to programming conventions. One such convention is
\textit{type hinting}, which is a way to explicitly note the type of a
variable. Type hinting does not have an effect on the code, but it makes the
code more readable and understandable for other developers, and allows for
development environments to detect type errors before execution
(\autoref{lst:type_hint})~\cite{vanrossumPEP484Type2014}.

\def\mycaption{
    Example of type hints used in python. Explicitly stating the type of the
    variable is optional and does not change the behavior of the code as shown in
    \autoref{lst:dynamic_typing}.}
\begin{lstlisting}[
    language=Python,
    style=pythonstyleNonbreaking,
    label=lst:type_hint,
    caption=\mycaption,
    ]
a: int = 5
a: str = 'Hello, World!'
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%

Python supports both functional and object-oriented programming paradigms. In
functional programming, the code is written in a way that the program is a
sequence of function calls, where each function call returns a value that is
used in the next function call (\autoref{lst:functional}). This approach is
useful when multiple actions have to be performed on the same data and the
structure of the data is relatively simple, for example a string of a gene
sequence.

\def\mycaption{ Example of functional programming in Python. The code
    defines a function called ``\texttt{find\_restriction\_site}'' that
    finds the position of a restriction site in a gene. The function
    ``\texttt{cut}'' uses the function ``\texttt{find\_restriction\_site}''
    to cut the gene at the restriction site.}
\begin{lstlisting}[
    language=Python,
    style=pythonstyleNonbreaking,
    label=lst:functional,
    caption=\mycaption,
    ]
def find_restriction_site(gene: str):
    return gene.find('GCGC')
    
def cut(gene: str):
    position = find_restriction_site(gene)
    return gene[position:]   
    
gene1 = 'TGAGCTGAGCTGATGCGCTATATTTAGGCG'
gene1_cut = cut(gene1)
print(gene1_cut)
# Output: GCGCTATATTTAGGCG
    
    
\end{lstlisting}


When the data itself gains in complexity, for example when storing not just the
gene sequence, but also the promotor sequence, an object-oriented approach is
more suitable (\autoref{lst:oop}). Object-oriented programming is a programming
paradigm that uses objects and classes. An object is a collection of both data
and functions, and a class is a blueprint for creating objects. The data of an
object is stored as attributes. Functions that are associated with an object are
called methods.

\def\mycaption{ Example of object oriented programming in python. The class is
    called ``\texttt{Gene}'' and has four methods, ``\texttt{\_\_init\_\_}'',
    ``\texttt{find\_promotor}'', ``\texttt{find\_restriction\_site}'' and
    ``\texttt{cut}''. The method ``\texttt{\_\_init\_\_}'' is called when
    creating (``initializing'') an object, which fills the object with
    user-defined data. The parameter ``\texttt{self}'' is used to reference the
    object itself internally. ``\texttt{find\_promotor}'' is a
    method that finds the position of the promotor in the gene and is called
    during object initialization. }
\begin{lstlisting}[
    language=Python,
    style=pythonstyle,
    label=lst:oop,
    caption=\mycaption,
    ]
class Gene:
    def __init__(self, sequence: str):
        self.sequence: str = sequence # Save sequence as attribute
        self.promotor: str = self.find_promotor() 
    def find_promotor(self):
        return self.sequence.find('TATA')
    def find_restriction_site(self):
        return self.sequence.find('GCGC')
    def cut(self):
        position = self.find_restriction_site()
        return self.sequence[position:]

gene1 = Gene(sequence='TGAGCTGAGCTGATGCGCTATATTTAGGCG') # Create object
gene1_cut = gene1.cut() # Call the method cut
print(gene1_cut) # Show result
# Output: GCGCTATATTTAGGCG
\end{lstlisting}

A major benefit of using an object oriented versus a functional approach is that
the data itself programmable, enabling the programmer to define the behavior of
the data itself through methods. This is achieved by using the keyword
``\texttt{self}'' to reference the object itself inside the class. For example,
one could extend the class ``\texttt{Gene}'' with a method that finds the
promotor of the gene and stores it as an attribute (\autoref{lst:oop}).

When designing software, both functional and object oriented programming can be
used together, where object oriented programming is often used to design the
program's overall architecture, and functional programming is used to implement
the algorithms of the program's features. This allows for scalability of the
software, as every single class is extended through the addition of new methods.
Furthermore, classes can be expanded in their functionalities through
inheritance (\autoref{lst:inheritance}).


\def\mycaption{ Example of inheritance in python.
    The class ``\texttt{mRNA}'' inherits from the class ``\texttt{Gene}''. The class
    ``\texttt{mRNA}'' has two methods, ``\texttt{\_\_init\_\_}'' and
    ``\texttt{find\_stopcodon}''. The method ``\texttt{find\_stopcodon}'' finds the
    position of stop codons. }
\begin{lstlisting}[
    language=Python,
    style=pythonstyle,
    label=lst:inheritance,
    caption=\mycaption,
    ]
# Define a class called mRNA inheriting from the class Gene
class mRNA(Gene):
    def __init__(self, sequence: str):
        super().__init__(sequence)  # Get attributes from parent class
        self.sequence.replace('T', 'U')  # Replace thymine with uracil
    def find_stopcodons(self):
        return self.sequence.find('UGA')

mrna1 = mRNA(sequence='TGAGCTGAGCTGATGCGCTATATTTAGGCG') # Create object
print(mrna1.find_stopcodons())  # Call the method translate
# Output: [0, 5, 10]
\end{lstlisting}

Inheritance is a feature of object-oriented programming that allows
a class to access every attribute and method of a parent class. For example, one
could extend the class ``\texttt{Gene}'' with a class ``\texttt{mRNA}'', by
writing a class ``\texttt{mRNA}'' that inherits from the class ``\texttt{Gene}''.

Together, python is not just beginner-friendly, but also well respected for its
ease in development, which is why it is widely used in professional settings for
web development, data analysis, machine learning, biosciences and more
\cite{ekmekciIntroductionProgrammingBioscientists2016,rayhanRisePythonSurvey2023}.

% ======================================================================
\unnsubsection{Data Science Packages with Python}
%%%
Python includes a vast number of built-in packages used for basic data-types,
software development, simple math operations, etc.,
\cite{PythonLanguageReference}. Still, python relies on packages developed by
its users to provide specialized tools for data analysis. A python package
consists of multiple python \emph{modules}, where each module is a text-file
with~a~\texttt{.py} ending containing python code. Famous examples of such
packages are \texttt{pytorch} and \texttt{tensorflow}, that are used to build
models of artificial intelligence, including \textit{ChatGPT}
\cite{paszkePyTorchImperativeStyle2019, abadiTensorFlowLargeScaleMachine2016,
    radfordLanguageModelsAre2019}. Here, we outlay the most important packages used
for \texttt{plotastic} in Chapter 2.

\texttt{IPython} can be understood as an enhanced version of the standard Python
interpreter, designed to improve the interactivity of Python code execution
\cite{perezIPythonSystemInteractive2007}. It streamlines the process of writing
and executing code by conveniently displaying rich media, visualization, and
extensive documentation, which helps in understanding complex data analyses.
This make \texttt{IPython} similar in functionality to \textit{MATLAB} or
\textit{RStudio}. This is particularly advantageous in the field of biomedicine,
where visualizing data trends and patterns can lead to significant insights.

\texttt{Jupyter} is an evolution of \texttt{IPython}, introducing the
\emph{Jupyter notebook} format (file-ending \texttt{.ipynb})
\cite{kluyverJupyterNotebooksPublishing2016}. Jupyter notebooks are documents
that combine both code and written text information and structures those into
\emph{code cells} and \emph{markdown cells}, respectively. These cells can be
executed individually, displaying the cell's output directly below the cell.
This allows for an interactive exploration of data, but also makes Jupyter
notebooks a very human-readable format that outlays data analysis in a clear
manner with precise and reproducible documentation of all data processing steps.
Jupyter notebooks have become a standard format compatible with collaborative
platforms like \textit{Google Colab} and \textit{JupyterLab}, but also
professional software development tools like \textit{VS Code}, and
\textit{PyCharm}.



\texttt{NumPy} accelerates the mathematical capabilities of Python by enabling
large-scale operations on multi-dimensional arrays and matrices with high
efficiency \cite{harrisArrayProgrammingNumPy2020}. One key feature of
\texttt{NumPy} is the implementation of SIMD (Single Instruction, Multiple Data)
instructions. SIMD allows multiple data points to be processed simultaneously,
significantly speeding up operations that are inherently parallelizable, such as
matrix addition or multiplication. This capability is crucial when handling the
vast datasets typically found in medical imaging or genomic data analysis in
cancer research.

\texttt{Pandas} extends Python with a tabular datatype, called
\texttt{DataFrame}, which allows for easy data manipulation with integrated
indexing \cite{mckinneyPandasFoundationalPython2011}. The intuitive interface of Pandas can be likened to Excel; however, it
is vastly more powerful due to its speed, functionality, and ability to handle
larger datasets. Unlike Excel, Pandas enables automation of data processing
scripts, which is indispensable for reproducible scientific analysis.

\texttt{Matplotlib} is a plotting library that provides a wide range of static,
animated, and interactive plots and graphs
\cite{hunterMatplotlib2DGraphics2007}. It serves as the foundation for many
other visualization tools and is particularly valued for its flexibility and
customization options. Researchers in biomedicine use Matplotlib to create a
variety of graphs (like histograms or scatter plots), which are essential for
preliminary data analysis and checking data distributions.

\texttt{Seaborn} builds on Matplotlib by integrating closely with
\texttt{Pandas} data structures and providing a high-level interface for drawing
attractive and informative statistical graphics
\cite{waskomSeabornStatisticalData2021}. It simplifies the creation of complex
visualizations involving multidimensional data, making it easier to reveal
patterns and relationships via color encoding, faceting, and automated
statistical fits. This is particularly useful in cancer research for visualizing
and understanding complex datasets, such as patient response variability to
treatments across multiple subgroups.

\texttt{Pingouin} is designed to be a user-friendly statistical tool that offers
a straightforward syntax for performing statistical tests, which are commonly
implemented in R \cite{vallatPingouinStatisticsPython2018}. Unlike R,
\texttt{Pingouin} integrates seamlessly within the Python ecosystem, which
allows combining data manipulation, analysis, and visualization all in one
platform. This integration is beneficial for researchers who wish to conduct
advanced statistical analysis without switching between different software
environments.

Together, these packages form the backbone of modern data analysis in Python,
often times combining software from different languages to accelerate certain
features, while retaining the ease of use and readability that Python is known
for. This is particularly advantageous in the field of biomedicine, where data
analysis is often complex and requires a high degree of flexibility and
customization.
